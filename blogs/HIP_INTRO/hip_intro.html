<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to HIP programming (AMD)</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />
    <link rel="stylesheet" type="text/css" href="../../style.css">
</head>
<body>
    <header>
        <h1>Introduction to HIP programming (AMD)</h1>
        <span class="date">Dec 6, 2025</span>
        <br><br>
        <a href="../../index.html" class="about-me-link">Home</a>
        &nbsp; &nbsp;
    </header>
    <main>
        <p>
            Before discussing the topic, let's talk about the hardware that runs these operations. GPUs (Graphics Processing Units) are specialized hardware designed to run multiple operations simultaneously for rendering games, 
            running simulations, and more. There are two major players in this race: AMD(Team Red) and NVIDIA(Team Green). Both compete neck-to-neck  when it comes to gaming, but the story is different in machine learning and compute. 
            NVIDIA has been dominating the AI market, until AMD introduced the <a href="https://www.youtube.com/watch?v=lVzVOs-5Qb0">RDNA4</a> and <a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf">CDNA4</a> architectures. 
            This architecture brings significant improvements in compute performance and AI workloads, featuring FP8 and INT4 with structured sparsity,  improved power efficiency, and better support for drivers on linux by offering competitive performance at potentially better price points, giving developers and 
            researchers more options when building AI systems and there is one more reason (personal choice) to choose AMD which is opensource ecosystsem unlike NVIDIA vendor locking of CUDA and its license constraints.
        </p>
        <br>
        <p>
            HIP (Heterogeneous-Compute Interface for Portability) is a C++ runtime API and kernel language that uses LLVM to provide an interface for AMD GPUs—similar to CUDA but less mature. HIP enables programming thousands 
            of GPU cores to execute parallel operations simultaneously. 
        </p>
        <p>
            Gpu's leverages the SIMD (Single Instruction, Multiple Data) programming model excels in <a href="https://en.algorithmica.org/hpc/pipelining/branchless/">branchless</a> programming techniques, where conditional 
            logic is minimized allowing the same operation to be performed across multiple data points simultaneously, maximizing GPU throughput.
        </p>
        <br>
        <p>
            Before the code and implementation lets understand some terminologies of HIP programming.
        </p>
        <br>
        <p>
            <img src="media/hip_intro_img_1.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>
        <br>
        <p>
            The HIP computation is ordered in three hierarchy level. Each initialization of hip code create a grid first then creates blocks and then threads. Threads which are in same block access the same memory region.
        </p>

        <p>
            <div>
                The GPU-accelerated computation follows a well-defined five-stage pipeline, illustrated below. Each stage plays a critical role in the overall execution efficiency.
            </div>
            <div style="font-family: monospace; white-space: pre; text-align: center;">
                [Host Memory] → [PCIe Transfer] → [Device Memory] → [Compute Kernels] <br>
                                                                            ↓<br>
                [Display Results] ← [Host Memory] ← [PCIe Transfer] ← [Device Memory]
            </div>

        </p>

        <h2>Problem Formulation</h2>
        <p>
            Given two matrices \( A \in \mathbb{R}^{m \times k} \) and \( B \in \mathbb{R}^{k \times n} \), we seek to compute their product \( C = A \times B \) where \( C \in \mathbb{R}^{m \times n} \), such that:
        </p>
        <p>
            \[
                C_{ij} = \sum_{p=1}^{k} A_{ip} \cdot B_{pj}
            \]
        </p>
        <p>
            for all \(i \in [1, m]\) and \(j \in [1, n]\).
        </p>
        <br>

        <h2>Naive Implementation</h2>

        

        <p>
            Naive implementation in matrix multiplication is the basic algorithm that computes each element of the result matrix as the sum of pairwise products from the input matrices' corresponding row and column
        </p>

        <h3>How It Works</h3>

        <p>
            <img src="media/hip_intro_img_2.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>

        <p>
            For matrices A (m × n) and B (n × p), the result C (m × p) has each entry C[i][j] calculated via a triple nested loop: for each i and j, sum A[i][k] * B[k][j] over k from 0 to n-1
        </p>


        <P>
            For Naive kernel, we’ll use the grid, block and thread hierarchy to assign each thread a unique entry in the result matrix C. Then that thread will compute the dot product of the corresponding row of A 
            and column of B, and write the result to C. Due to each location of C being written to by only one thread, we  don't have to do any synchronization
        </P>

        <pre class="line-numbers">
            <code class="language-cpp">
                /*
                * @info - The function multiply two matrix of size A[MxK] and B[KxN] which will return a matrix C[MxN]
                * @param - A and B are constant matrix and C will be the output matrix
                * @param - M, N and K are the size of matrix rows and cols w.r.t matrix   
                */
                __global__ void matrixMultiplication(const float* A, const float* B, float* C, int M, int N, int K) {
                  int row = threadIdx.y + blockIdx.y * blockDim.y;
                  int col = threadIdx.x + blockIdx.x * blockDim.x;
                  
                  if (row < M && col < N) {
                    float sum = 0.0f;
                    for(int i = 0; i < K; ++i) {
                      sum += A[row * K + i] * B[i * N + col];
                    }
                    C[row * N + col] = sum;
                  }
                }

            </code>
        </pre>

        <p>
            The Code mention below the compute function for each thread in the computation
            <img src="media/threads2.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>

        <p>
            While the naive implementation demonstrates the fundamental concepts of HIP programming and parallel matrix multiplication, it's important to recognize that this is merely the starting point. The naive kernel, though functionally correct, suffers from significant performance bottlenecks that prevent it from achieving optimal GPU utilization.
        </p>
        
        <p>
            The primary limitation lies in memory access patterns. Each thread repeatedly accesses global memory for elements of matrices A and B, resulting in poor memory bandwidth utilization and high latency. Modern GPUs have complex memory hierarchies including shared memory, L1 and L2 caches, and registers, which the naive implementation fails to exploit effectively. Additionally, there's no memory coalescing optimization, meaning adjacent threads may access non-contiguous memory locations, further degrading performance.
        </p>

        <!-- <h4>Whole Code</h4>

        <p>
            Note: Please replace "" with `<>`
        </p>

        <pre class="line-numbers">
            <code class="language-cpp">
                #include "cstdlib"
                #include "hip/hip_runtime.h"
                #include "iostream"
                #include "vector"
                
                #define HIP_CHECK(exp) do {                                                                                        \
                  const hipError_t err = exp;                                                                                      \
                  if (err != hipSuccess) {                                                                                         \
                    std::cerr << "Hip error: " << hipGetErrorString(err) << " at " << __FILE__ << " : " << __LINE__ << std::endl;  \
                    std::exit(EXIT_FAILURE);                                                                                       \
                  }                                                                                                                \
                } while(0)
                
                /*
                * @info - The function multiply two matrix of size A[MxK] and B[KxN] which will return a matrix C[MxN]
                * @param - A and B are constant matrix and C will be the output matrix
                * @param - M, N and K are the size of matrix rows and cols w.r.t matrix   
                */
                __global__ void matrixMultiplication(const float* A, const float* B, float* C, int M, int N, int K) {
                  int row = threadIdx.y + blockIdx.y * blockDim.y;
                  int col = threadIdx.x + blockIdx.x * blockDim.x;
                  
                  if (row < M && col < N) {
                    float sum = 0.0f;
                    for(int i = 0; i < K; ++i) {
                      sum += A[row * K + i] * B[i * N + col];
                    }
                    C[row * N + col] = sum;
                  }
                }
                
                /*
                * Function - initMatrix
                * @info - The function create a matrix at host(cpu)
                */
                void initMatrix(std::vector<float> &vec, int rows, int cols) {
                  for (int i = 0; i < rows * cols; ++i) {
                    vec[i] = static_cast<float>(i + 1);
                  }
                }
                
                void run_kernel() {
                  int M = 64;  // Reduced size for easier debugging
                  int N = 64;
                  int K = 64;
                  
                  // Allocate cpu memory
                  std::vector<float> h_A(M * K);
                  std::vector<float> h_B(K * N);
                  std::vector<float> h_C(M * N);
                  
                  
                  initMatrix(h_A, M, K);
                  initMatrix(h_B, K, N); 
                
                  
                  // Allocate gpu memory
                  float *d_A, *d_B, *d_C;
                  HIP_CHECK(hipMalloc(&d_A, M * K * sizeof(float)));
                  HIP_CHECK(hipMalloc(&d_B, K * N * sizeof(float)));
                  HIP_CHECK(hipMalloc(&d_C, M * N * sizeof(float)));
                  
                  // copy data from host(cpu) to gpu
                  HIP_CHECK(hipMemcpy(d_A, h_A.data(), M * K * sizeof(float), hipMemcpyHostToDevice));
                  HIP_CHECK(hipMemcpy(d_B, h_B.data(), K * N * sizeof(float), hipMemcpyHostToDevice));
                  
                  // Launch kernel
                  dim3 blockSize(16, 16);
                  dim3 gridSize((N + blockSize.x - 1) / blockSize.x, (M + blockSize.y - 1) / blockSize.y);
                
                  // Hip style code 
                  void *kernelArgs[] = { &d_A, &d_B, &d_C, &M, &N, &K }; 
                  HIP_CHECK(hipLaunchKernel((const void*)matrixMultiplication, gridSize, blockSize, (void**)kernelArgs, 0, 0));
                  
                  // Cuda style code
                  //matrixMultiplication<<<gridSize, blockSize>>>(d_A, d_B, d_C, M, N, K);
                
                  // Check for error while launching kernel
                  HIP_CHECK(hipGetLastError());
                  
                  HIP_CHECK(hipDeviceSynchronize());  
                  
                  // Copy result back from GPU to CPU
                  HIP_CHECK(hipMemcpy(h_C.data(), d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));
                  
                  std::cout << "\nResult Matrix C (" << M << "x" << N << "):" << std::endl;
                  for(int i = 0; i < M; ++i) {
                    for(int j = 0; j < N; ++j) {
                      std::cout << h_C[i * N + j] << " ";
                    }
                    std::cout << std::endl;
                  }
                  
                  // Clean up
                  HIP_CHECK(hipFree(d_A));
                  HIP_CHECK(hipFree(d_B));
                  HIP_CHECK(hipFree(d_C));  
                }
                
                int main() {
                  run_kernel();
                  return 0;
                }
            </code>
        </pre>


        <br> -->
        <br>

        <h2> Shared Memory Cache </h2>

        <p>
            Before understanding shared memory lets understand the concept of wavefront(Rocm) / warp(CUDA) these are fundamental hardware units which are used to schedule
            the threads in the batch of 32, the wavefront follows SIMT(Same Instruction Multiple Threads) format this means all the threads in same wavefront executes the
            same instructions.
        </p>

        <h4> How Scheduling and Execution Works</h4>

        <p>
            When we launch any gpu kernel with threads organized into blocks the gpu automatically divides those threads into wavefront of size 32
        </p>

        <p>
            <img src="media/wavefrontscheduling.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>

        <p>
            The wavefront scheduler picks wavefront that are ready to execute the issued instruction sets to them
            , all the 32 threads in wavefront executes the same instruction set
        </p>


        <p>
            <img src="media/wavefrontinfo.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>

        <p>
            Lets move on to the main topic which is shared memory, The GPU i posses have four types of memory: a) L1 Cache/Shared memory
            b) L2 Cache , c) L3 Cache and d) Global Memory (DRAM)
        </p>

        <p>
            <img src="media/gpu_cache_info.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>

        <p>
            * L1 Cache / Shared Memory :- This is type of memory which is shared by same compute uint in my case its 32 kB which gives
            pretty fast access of data which is 5ns
        </p>

        <p>
            * L2 Cache :- This is type of memory which is shared by all compute units on the die which in my case is 8MB on die L2 Cache
        </p>

        <p>
            * L3 Cache(Infinity Cache in RDNA4) :- The cache sits b/w L2 cache and Global Memory(GDDR6 DRAM) acting as buffer that reduce
            the need to access slower, power hungry off chip memory. This new set of memory architecture really level up the game of AMD.
            With this kind of memory architecture you can use streams effectively for to-and-fro of data.
        </p>


        <p>
            For this kernel we will load the chunks of A and chunks of B from global memory to the shared
            memory. Then we perform all the operations using shared memory, we will move chunks along with
            A and B doing partial sums
        </p>

        <h3> How it works </h3>

        <p>
            <img src="media/shared_mem_implemntation.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
        </p>


        <pre class="line-numbers">
            <code class="language-cpp">
                template "<"const uint BLOCKSIZE = 32, typename T">"
                __global__ void matmulKernel3(T const* A, T const* B, T* C, uint M, uint N, uint K) {
                    const uint cRow = blockIdx.x;
                    const uint cCol = blockIdx.y;
                
                    __shared__ float As[BLOCKSIZE * BLOCKSIZE];
                    __shared__ float Bs[BLOCKSIZE * BLOCKSIZE];
                
                    int threadCol = threadIdx.x % BLOCKSIZE;
                    int threadRow = threadIdx.x / BLOCKSIZE;
                
                    A += cRow * BLOCKSIZE * K;
                    B += cCol * BLOCKSIZE;                        
                    C += cRow * BLOCKSIZE * N + cCol * BLOCKSIZE; 
                
                    float sum = 0.0;
                
                    for (int bkIdx = 0; bkIdx < K; bkIdx += BLOCKSIZE) {
                        As[threadRow * BLOCKSIZE + threadCol] = A[threadRow * K + threadCol];
                        Bs[threadRow * BLOCKSIZE + threadCol] = B[threadRow * N + threadCol];
                
                        __syncthreads();
                
                        A += BLOCKSIZE;
                        B += BLOCKSIZE * N;
                
                        for (int dotIdx = 0; dotIdx < BLOCKSIZE; ++dotIdx) {
                            sum += As[threadRow * BLOCKSIZE + dotIdx] * Bs[dotIdx  * BLOCKSIZE + threadCol];
                        }
                
                        __syncthreads();
                    }
                
                    C[threadRow * N + threadCol] = sum;
                
                }
            </code>
        </pre>

    <p>
        After comparing the performance of second kernel over first we got about 5x in performance increase in kernel execution.
        The Naive Kernel took about 377ms to execute 4096 x 4096 matrix multiplication and for same size of matrix Shared memory
        version of kernel tool 81ms which is huge jump in performance
    </p>


    <h4> Naive Kernel Trace </h4>
    
    <p>
        <img src="media/trace_navie_kernel.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
    </p>

    <h4>Shared Memory Kernel</h4>

    <p>
        <img src="media/trace_shared_mem.png" style="max-width: 100%; max-height: 500px; width: auto; height: auto; display: block; margin: 0 auto;">
    </p>


    <h3>Next Kernels Coming Soon...</h3>
        
    </main>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/line-numbers/prism-line-numbers.min.js"></script>
</body>